{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Cópia de 00_carga_df_api_01.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj9Em9dXrUbC"
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import janitor\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell \n",
        "InteractiveShell.ast_node_interactivity=\"all\"\n",
        "\n",
        "# Set API urls\n",
        "url_all_country = 'https://api.covid19api.com/countries'\n",
        "summary_url = 'https://api.covid19api.com/summary'\n",
        "\n",
        "def request_url(urls, return_response = False):\n",
        "    '''\n",
        "    Function to make request given an url.\n",
        "    \n",
        "    INPUT:\n",
        "        urls - String of a given API address. \n",
        "    OUTPUT:\n",
        "        r - String of response in Json  format.\n",
        "    '''\n",
        "    try:\n",
        "        r = requests.get(url=urls)\n",
        "        if return_response:\n",
        "            return r\n",
        "        return r.json()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(f\"Houve um erro ao tentar fazer a requisicao da url: {urls}\")\n",
        "\n",
        "\n",
        "def request_data_by_country(country_name, cases = [\"confirmed\",\"deaths\"]):\n",
        "    '''\n",
        "    Function to make request and get response for more than one country in a given standard API URL\n",
        "    since 2020-01-01.\n",
        "    \n",
        "    INPUT:\n",
        "        country_name - List of String containing country Slugs.\n",
        "        cases(Optional) - List of Strings containing type of update \"confirmed\",\"deaths\" or \"recovery\".\n",
        "    OUTPUT:\n",
        "        json_list - String of response in Json  format.\n",
        "        country_request_err - List of String with Slugs that did not reach an API response or returned an error.\n",
        "        \n",
        "    '''\n",
        "    today = datetime.datetime.utcnow().strftime('%Y-%m-%dT00:00:00Z')\n",
        "    json_list = []\n",
        "    country_request_reprocess = []\n",
        "    \n",
        "    for name in country_name:\n",
        "        for case_type in cases:\n",
        "            try:\n",
        "                print(f'pais: {name} e tipo de caso:{case_type}')\n",
        "                counter = 0\n",
        "                urls = f'https://api.covid19api.com/country/{name}/status/{case_type}?from=2020-01-01T00:00:00Z&to={today}'\n",
        "                response = request_url(urls, True)\n",
        "                \n",
        "                # Try to make the request 5 times if you don't get a response the first time\n",
        "                while response.status_code != 200:\n",
        "                    counter += 1\n",
        "                    response = request_url(urls, True)\n",
        "                    time.sleep(30)\n",
        "                    if counter == 5:\n",
        "                        break\n",
        "                \n",
        "                if len(response) > 0:\n",
        "                    json_list.append(response.json())\n",
        "                else:\n",
        "                    country_request_reprocess.append(name)\n",
        "                \n",
        "                # timer to avoid request limit  \n",
        "                time.sleep(6*60)\n",
        "            except Exception as e:\n",
        "                country_request_reprocess.append(name)\n",
        "                print(e)\n",
        "                print(f\"Houve um erro ao tentar fazer a requisicao da url com o pais: {name} e tipo de caso:{case_type}\")\n",
        "        \n",
        "    return json.dumps(json_list), country_request_reprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_u79Mu6sJYk"
      },
      "source": [
        "def get_top_deaths_confirmed_contries(summary_dataframe, country_dataframe, n = 30):\r\n",
        "    '''\r\n",
        "    Function get top countries in deaths and confirmed cases.\r\n",
        "    \r\n",
        "    INPUT:\r\n",
        "        summary_dataframe - Dataframe cointaing a summary of all COVID-19 cases.\r\n",
        "        country_dataframe - Dataframe cointaing name of country, slug and ISO2 code.\r\n",
        "        n (optional) - Integer number refered to top countries to return.\r\n",
        "    OUTPUT:\r\n",
        "        top_countries - List of strings containg top countries in both categories.\r\n",
        "    '''\r\n",
        "    \r\n",
        "    # top rank Countries by confirmed cases\r\n",
        "    top_TotalConfirmed = summary_dataframe.sort_values(by=['TotalConfirmed'], ascending = False).head(n)\r\n",
        "    list_top_confirmed = list(top_TotalConfirmed['CountryCode'].unique())\r\n",
        "\r\n",
        "    # top rank Countries by death\r\n",
        "    top_deaths = summary_dataframe.sort_values(by=['TotalDeaths'], ascending = False).head(n)\r\n",
        "    list_top_deaths = list(top_deaths['CountryCode'].unique())\r\n",
        "\r\n",
        "    # List with tops Death and Confirmed cases\r\n",
        "    countries_ISO = list(set(list_top_confirmed+list_top_deaths))\r\n",
        "\r\n",
        "    # get name of countries in Slug\r\n",
        "    top_countries = list(country_dataframe[country_dataframe['ISO2'].isin(countries_ISO)]['Slug'].unique())\r\n",
        "    \r\n",
        "    return top_countries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixbx6eaLsKKS"
      },
      "source": [
        "def build_dataframe(json_agg):\r\n",
        "    '''\r\n",
        "    Function to consolidate all countries in json into a single dataframe.\r\n",
        "    \r\n",
        "    INPUT:\r\n",
        "        json_agg - list of Json containg data from all top countries.   \r\n",
        "    OUTPUT:\r\n",
        "        dataframe_top_contries - String of response in Json  format.\r\n",
        "    '''\r\n",
        "    # iterate thru json and get all data \r\n",
        "    dataframe_top_contries = pd.DataFrame()\r\n",
        "    for element in json_agg:\r\n",
        "        dataframe_top_contries = dataframe_top_contries.append(pd.json_normalize(element), ignore_index=True)\r\n",
        "    \r\n",
        "    return dataframe_top_contries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nRJUNByrUbH"
      },
      "source": [
        "import psycopg2\n",
        "class Postgres():\n",
        "  def __init__(self,database='postgres',user='DataWarriorsAdmin@data-warriors-postgresql',password='DataWarriorsPassword!',host='data-warriors-postgresql.postgres.database.azure.com',port='5432'):\n",
        "    self.connection = psycopg2.connect(host=host, database=database,user=user, password=password,port=port, sslmode='require')\n",
        "    self.new_cursor()\n",
        "\n",
        "  def get_connection(self):\n",
        "    return self.connection\n",
        "\n",
        "  def new_cursor(self):\n",
        "    self.cursor = self.connection.cursor()\n",
        "    return self.cursor\n",
        "\n",
        "  def commit(self):\n",
        "    self.connection.commit()\n",
        "\n",
        "  def rollback(self):\n",
        "    self.connection.rollback()\n",
        "\n",
        "  def fetchall(self):\n",
        "    return self.cursor.fetchall()\n",
        "\n",
        "  def execute(self,query):\n",
        "    self.cursor.execute(query)\n",
        "    self.commit()\n",
        "\n",
        "  def search(self,query):\n",
        "    self.new_cursor()\n",
        "    self.cursor.execute(query)\n",
        "    self.commit()\n",
        "    return self.fetchall()\n",
        "\n",
        "  def insertMany(self,query,data_list):\n",
        "    try:\n",
        "      self.new_cursor()\n",
        "      self.cursor.executemany(query, data_list)\n",
        "      self.commit()    \n",
        "      print('Execução da transação conclída com sucesso:', query)\n",
        "    except Exception as error:\n",
        "      self.rollback()\n",
        "      print('Transação falhou:',error)\n",
        "    \n",
        "  def closeConnection(self):\n",
        "    self.connection.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3YO1_w2rUbI"
      },
      "source": [
        "import json\n",
        "class ParseDFToDatabase(object):\n",
        "  def __init__(self,df,table_name):\n",
        "    self.df = df\n",
        "    self.table_name = table_name\n",
        "    self.tuple_list = self.dfToTupleList()\n",
        "    self.columns = tuple(df.columns.to_list())\n",
        "    self.format_colums = str(tuple([''.join('%s') for column in self.columns])).replace(\"'\",\"\")\n",
        "    self.columns = str(tuple(self.columns)).replace(\"'\",\"\").upper()\n",
        "    self.insert_query = f'INSERT INTO {self.table_name}{self.columns} VALUES{self.format_colums}'\n",
        "  \n",
        "  def dfToTupleList(self):\n",
        "    df_json = json.loads(self.df.to_json(orient='records'))\n",
        "    return list(map(lambda row: tuple(row.values()),df_json ))\n",
        "\n",
        "  def get(self):\n",
        "    return self.insert_query, self.tuple_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAE6lbANrUbI"
      },
      "source": [
        "# request countries\n",
        "data_country = pd.json_normalize(request_url(url_all_country))\n",
        "\n",
        "# request summary\n",
        "data_sum = request_url(summary_url)\n",
        "top = pd.json_normalize(data_sum['Countries'])\n",
        "\n",
        "# get a list containing top 30 countries with confirmed and death cases\n",
        "list_top_countries = get_top_deaths_confirmed_contries(top,data_country)\n",
        "\n",
        "# make requestget and aggregate all top 30 countries in a single json\n",
        "aggregate, list_to_reprocess = request_data_by_country(list_top_countries)\n",
        "aggregate_json = json.loads(aggregate)\n",
        "\n",
        "# save log file\n",
        "today = datetime.datetime.utcnow().strftime('%Y-%m-%dT00:00:00Z')\n",
        "with open(f'error_logs_{today}.txt', 'a') as writer:\n",
        "    writer.writelines(f'Countries that request failed: {str(list_to_reprocess)} | Process date: {today}')\n",
        "\n",
        "# make a datafrane of json file\n",
        "df_top_30_by_cases = build_dataframe(aggregate_json)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzJ-QFH5rUbI"
      },
      "source": [
        "def preparaInsercaoDaily(df):\n",
        "    df.drop(columns=['Country', 'Province', \n",
        "                 'City', 'CityCode', 'Lat', 'Lon',\"message\"], inplace=True)\n",
        "    df[\"TOTALDEATHS\"]=df[\"Cases\"][df[\"Status\"]==\"deaths\"]\n",
        "    df[\"TOTALDEATHS\"]=df[\"Cases\"][df[\"Status\"]==\"deaths\"]\n",
        "    df[\"TOTALCONFIRMED\"]=df[\"Cases\"][df[\"Status\"]==\"confirmed\"]\n",
        "    df.fillna(0, inplace=True)\n",
        "    df.drop(columns=[\"Cases\",\"Status\"], inplace=True)\n",
        "    df=df.groupby([\"Date\",\"CountryCode\"])[[\"TOTALDEATHS\",\"TOTALCONFIRMED\"]].sum()\n",
        "    df=df.reset_index()\n",
        "    df=df.rename({\"Date\":\"DATEREG\",\n",
        "          \"CountryCode\":\"COUNTRYCODE\"},\n",
        "         axis=1)\n",
        "    df[\"DATEREG\"]=pd.to_datetime(df['DATEREG']).dt.date\n",
        "    df[\"DATEREG\"]=df[\"DATEREG\"].astype(str)\n",
        "    grouped=df.groupby('COUNTRYCODE')\n",
        "    df_novo=pd.DataFrame()\n",
        "    for name, country in grouped:\n",
        "        country[\"NEWCONFIRMED\"]=country[\"TOTALCONFIRMED\"].diff(1)\n",
        "        country[\"NEWDEATHS\"]=country[\"TOTALDEATHS\"].diff(1)\n",
        "        df_novo=pd.concat([df_novo,country], ignore_index=True)\n",
        "    df=df_novo.fillna(0)\n",
        "    df=df.astype({\"TOTALCONFIRMED\":int, \"TOTALDEATHS\":int, \"NEWCONFIRMED\":int, \"NEWDEATHS\":int})\n",
        "    df=df[['DATEREG', \n",
        "       'COUNTRYCODE', \n",
        "       'TOTALCONFIRMED',\n",
        "       'TOTALDEATHS', \n",
        "       'NEWCONFIRMED', \n",
        "       'NEWDEATHS']]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cna58oVqrUbJ"
      },
      "source": [
        "def preparaInsercaoSummary():\n",
        "    grouped=df.groupby('COUNTRYCODE')\n",
        "    df_summary=pd.DataFrame()\n",
        "    for name, country in grouped:\n",
        "        reg=country.tail(1)\n",
        "        df_summary=pd.concat([df_summary,reg], ignore_index=True)\n",
        "    df_summary=df_summary.rename({\"DATEREG\":\"LASTUPDATED\"}, \n",
        "                    axis=1)\n",
        "    \n",
        "    return df_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE1myxfLrUbJ"
      },
      "source": [
        "def fazInsercao(df,tabela,banco):\n",
        "    x=ParseDFToDatabase(df,tabela)\n",
        "    query=x.get()[0]\n",
        "    dados=x.get()[1]\n",
        "    banco.insertMany(query,dados)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0YZbbqssfB6"
      },
      "source": [
        "### Parte guto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H83DqUkysYZf"
      },
      "source": [
        "# request countries\r\n",
        "data_country = pd.json_normalize(request_url(url_all_country))\r\n",
        "\r\n",
        "# request summary\r\n",
        "data_sum = request_url(summary_url)\r\n",
        "top = pd.json_normalize(data_sum['Countries'])\r\n",
        "\r\n",
        "# get a list containing top 30 countries with confirmed and death cases\r\n",
        "list_top_countries = get_top_deaths_confirmed_contries(top,data_country)\r\n",
        "\r\n",
        "# make requestget and aggregate all top 30 countries in a single json\r\n",
        "aggregate, list_to_reprocess = request_data_by_country(list_top_countries)\r\n",
        "aggregate_json = json.loads(aggregate)\r\n",
        "\r\n",
        "# make a datafrane of json file\r\n",
        "df_top_30_by_cases = build_dataframe(aggregate_json)\r\n",
        "df = f_top_30_by_cases.dropna(subset = ['Country'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWK3efqDshuZ"
      },
      "source": [
        "### Parte Carlos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW8EXAfwrUbK"
      },
      "source": [
        "# Prepara o dataframe gerado pelo acesso à api para inserção na base\n",
        "#\n",
        "df=preparaInsercaoDaily(df)\n",
        "\n",
        "# Prepara o dataframe df_summary para inserção na base\n",
        "df_summary=preparaInsercaoSummary(df)\n",
        "\n",
        "#Connecta no banco\n",
        "banco=Postgres()\n",
        "conn=banco.get_connection()\n",
        "cursor=banco.new_cursor()\n",
        "\n",
        "fazInsercao(df,\"DAILY\",banco)\n",
        "fazInsercao(df_summary,\"SUMMARY\",banco)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWumUGutzPnP"
      },
      "source": [
        "SELECT datereg from public.daily \r\n",
        "order by datereg desc \r\n",
        "limit 1;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y99Zk3lizaz8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}